{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T18:10:09.210685Z",
     "iopub.status.busy": "2025-06-02T18:10:09.209876Z",
     "iopub.status.idle": "2025-06-02T18:15:00.493672Z",
     "shell.execute_reply": "2025-06-02T18:15:00.493092Z",
     "shell.execute_reply.started": "2025-06-02T18:10:09.210653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\treehop\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Installing dependencies...\n",
      "🎯 === CORRECTED TreeHop Multi-hop QA ===\n",
      "📁 Looking for files:\n",
      "  Corpus: d:\\VDT2025\\MultiHopRag\\multihoprag_corpus.txt\n",
      "  Queries: d:\\VDT2025\\MultiHopRag\\MultiHopRAG.json\n",
      "\n",
      "📖 Loading corpus and queries...\n",
      "📖 Loading corpus from d:\\VDT2025\\MultiHopRag\\multihoprag_corpus.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing corpus: 57414it [00:00, 1277360.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 609 documents\n",
      "📖 Loading queries from d:\\VDT2025\\MultiHopRag\\MultiHopRAG.json...\n",
      "✅ Loaded 2556 queries\n",
      "✅ Loaded 609 documents and 2556 queries\n",
      "\n",
      "📝 Preparing passages...\n",
      "✅ Created passages file at d:\\VDT2025\\MultiHopRag\\passages.jsonl\n",
      "\n",
      "🧠 Initializing CORRECTED TreeHop...\n",
      "🚀 Initializing CORRECTED TreeHop Retriever:\n",
      "  embed_dim: 1024, g_size: 64\n",
      "  n_heads: 3, mlp_size: 64\n",
      "🔧 Loading BGE-m3...\n",
      "⚠️ Using fallback embedding\n",
      "✅ Initialized 3 CORRECTED attention heads\n",
      "\n",
      "🤖 Setting up Gemini API...\n",
      "📊 Split into 2044 train and 512 test queries\n",
      "\n",
      "🎯 Running CORRECTED TreeHop evaluation...\n",
      "🎯 Running evaluation on 10 samples from 512 total queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test queries:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/10] Processing query: What company, recently discussed in TechCrunch articles, is not only facing criticism for its new ge...\n",
      "✅ Loaded 609 passages\n",
      "🔍 Starting CORRECTED TreeHop 3-hop retrieval for 1 queries\n",
      "\n",
      "=== Processing Query 1/1 ===\n",
      "Query: Which company, recently featured in TechCrunch articles, is facing criticism for its generative AI m...\n",
      "✅ Initial query embedding shape: (1024,)\n",
      "\n",
      "--- Hop 1/3 ---\n",
      "🔄 Encoding passages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 609/609 [00:00<00:00, 26135.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoded 609 passages\n",
      "✅ Retrieved 3 passages\n",
      "🧠 Applying CORRECTED TreeHop graph-based message passing...\n",
      "  Before: query norm = 1.0000\n",
      "  Selected query branch 1/3 (score: 1.4196)\n",
      "  After: query norm = 1.0000\n",
      "  Query change = 1.3059\n",
      "  Overlap magnitude = 0.3518\n",
      "\n",
      "--- Hop 2/3 ---\n",
      "  Layer-wise pruning: using top-3 for hop 2\n",
      "✅ Retrieved 3 passages\n",
      "🧠 Applying CORRECTED TreeHop graph-based message passing...\n",
      "  Before: query norm = 1.0000\n",
      "  Selected query branch 1/3 (score: 0.9290)\n",
      "  After: query norm = 1.0000\n",
      "  Query change = 0.8810\n",
      "  Overlap magnitude = 1.2020\n",
      "\n",
      "--- Hop 3/3 ---\n",
      "  Layer-wise pruning: using top-3 for hop 3\n",
      "✅ Retrieved 3 passages\n",
      "\n",
      "✅ Query 1 complete:\n",
      "  Hops: 3\n",
      "  Total passages: 9\n",
      "  Unique passages: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting CORRECTED TreeHop 1-hop retrieval for 1 queries\n",
      "\n",
      "=== Processing Query 1/1 ===\n",
      "Query: Retrieve TechCrunch articles that mention companies facing criticism for generative AI models, YouTu...\n",
      "✅ Initial query embedding shape: (1024,)\n",
      "\n",
      "--- Hop 1/1 ---\n",
      "✅ Retrieved 3 passages\n",
      "\n",
      "✅ Query 1 complete:\n",
      "  Hops: 1\n",
      "  Total passages: 3\n",
      "  Unique passages: 3\n",
      "🤖 Generating answer with Gemini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test queries:  10%|█         | 1/10 [00:32<04:56, 32.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/10] Processing query: Which company, known for spending $26.3 billion in 2021 to secure its position as the default search...\n",
      "🔍 Starting CORRECTED TreeHop 3-hop retrieval for 1 queries\n",
      "\n",
      "=== Processing Query 1/1 ===\n",
      "Query: Which search engine company, criticized for anti-competitive practices like paying to be the default...\n",
      "✅ Initial query embedding shape: (1024,)\n",
      "\n",
      "--- Hop 1/3 ---\n",
      "✅ Retrieved 3 passages\n",
      "🧠 Applying CORRECTED TreeHop graph-based message passing...\n",
      "  Before: query norm = 1.0000\n",
      "  Selected query branch 1/3 (score: 1.4307)\n",
      "  After: query norm = 1.0000\n",
      "  Query change = 1.3125\n",
      "  Overlap magnitude = 0.3614\n",
      "\n",
      "--- Hop 2/3 ---\n",
      "  Layer-wise pruning: using top-3 for hop 2\n",
      "✅ Retrieved 3 passages\n",
      "🧠 Applying CORRECTED TreeHop graph-based message passing...\n",
      "  Before: query norm = 1.0000\n",
      "  Selected query branch 2/3 (score: 0.9095)\n",
      "  After: query norm = 1.0000\n",
      "  Query change = 0.8632\n",
      "  Overlap magnitude = 1.2196\n",
      "\n",
      "--- Hop 3/3 ---\n",
      "  Layer-wise pruning: using top-3 for hop 3\n",
      "✅ Retrieved 3 passages\n",
      "\n",
      "✅ Query 1 complete:\n",
      "  Hops: 3\n",
      "  Total passages: 9\n",
      "  Unique passages: 9\n",
      "🔍 Starting CORRECTED TreeHop 1-hop retrieval for 1 queries\n",
      "\n",
      "=== Processing Query 1/1 ===\n",
      "Query: Retrieve information about search engine companies and antitrust allegations, specifically focusing ...\n",
      "✅ Initial query embedding shape: (1024,)\n",
      "\n",
      "--- Hop 1/1 ---\n",
      "✅ Retrieved 3 passages\n",
      "\n",
      "✅ Query 1 complete:\n",
      "  Hops: 1\n",
      "  Total passages: 3\n",
      "  Unique passages: 3\n",
      "🤖 Generating answer with Gemini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test queries:  20%|██        | 2/10 [01:05<04:19, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/10] Processing query: Did the 'Business Line' article published after the 'The Guardian' article suggest that Leqembi (lec...\n",
      "🔍 Starting CORRECTED TreeHop 3-hop retrieval for 1 queries\n",
      "\n",
      "=== Processing Query 1/1 ===\n",
      "Query: Did the 'Business Line' article, published after 'The Guardian's' coverage of dementia research, imp...\n",
      "✅ Initial query embedding shape: (1024,)\n",
      "\n",
      "--- Hop 1/3 ---\n",
      "✅ Retrieved 3 passages\n",
      "🧠 Applying CORRECTED TreeHop graph-based message passing...\n",
      "  Before: query norm = 1.0000\n",
      "  Selected query branch 1/3 (score: 1.4132)\n",
      "  After: query norm = 1.0000\n",
      "  Query change = 1.3052\n",
      "  Overlap magnitude = 0.3539\n",
      "\n",
      "--- Hop 2/3 ---\n",
      "  Layer-wise pruning: using top-3 for hop 2\n",
      "✅ Retrieved 3 passages\n",
      "🧠 Applying CORRECTED TreeHop graph-based message passing...\n",
      "  Before: query norm = 1.0000\n",
      "  Selected query branch 2/3 (score: 0.8903)\n",
      "  After: query norm = 1.0000\n",
      "  Query change = 0.8315\n",
      "  Overlap magnitude = 1.2681\n",
      "\n",
      "--- Hop 3/3 ---\n",
      "  Layer-wise pruning: using top-3 for hop 3\n",
      "✅ Retrieved 3 passages\n",
      "\n",
      "✅ Query 1 complete:\n",
      "  Hops: 3\n",
      "  Total passages: 9\n",
      "  Unique passages: 9\n",
      "⚠️  LLM call failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "⚠️  LLM call failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test queries:  20%|██        | 2/10 [01:15<05:03, 37.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Critical Error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25596\\531671586.py\", line 1323, in main\n",
      "    results, metrics = run_evaluation(\n",
      "                       ~~~~~~~~~~~~~~^\n",
      "        retriever, qa_model, test_queries,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        n_hop=MAX_HOPS, top_n=TOP_N,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        max_samples=max_test_samples\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25596\\531671586.py\", line 1026, in run_evaluation\n",
      "    result = multihop_qa_llm(retriever, qa_model, query_data,\n",
      "                     n_hop=n_hop, top_n=top_n)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25596\\531671586.py\", line 1205, in multihop_qa_llm\n",
      "    run_q, summaries, passages = enhanced_multihop_search(\n",
      "                                 ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        retriever, qa_model, query,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        use_summary   = USE_PASSAGE_SUM\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25596\\531671586.py\", line 1180, in enhanced_multihop_search\n",
      "    passages = rerank_passages(llm_model, run_query, passages, top_k=top_n)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25596\\531671586.py\", line 1125, in rerank_passages\n",
      "    score = float(score_txt.split()[0])\n",
      "                  ~~~~~~~~~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "TreeHop Multi-hop QA - CORRECTED VERSION  \n",
    "Fixed according to original TreeHop repo analysis\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "from collections import defaultdict, namedtuple\n",
    "from statistics import mean, stdev\n",
    "from typing import List, Dict, Tuple, Union, Set, Optional\n",
    "\n",
    "# Setup paths for Kaggle\n",
    "KAGGLE_INPUT_DIR = \"/kaggle/input\"\n",
    "DATASET_DIR = os.path.join(KAGGLE_INPUT_DIR, \"my-treehop-data\")\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# File names\n",
    "CORPUS_FILENAME = \"multihoprag_corpus.txt\"\n",
    "QUERIES_FILENAME = \"MultiHopRAG.json\"\n",
    "\n",
    "# Add paths for local environment\n",
    "IS_KAGGLE = os.path.exists(KAGGLE_INPUT_DIR)\n",
    "if not IS_KAGGLE:\n",
    "    BASE_DIR = os.getcwd()\n",
    "    DATASET_DIR = BASE_DIR\n",
    "    OUTPUT_DIR = BASE_DIR\n",
    "\n",
    "# Configuration\n",
    "MAX_HOPS = 3\n",
    "TOP_N = 3\n",
    "REDUNDANT_PRUNING = True\n",
    "LAYERWISE_TOP_PRUNING = True\n",
    "TRAIN_RATIO = 0.8\n",
    "SEED = 42\n",
    "GEMINI_MODEL = 'gemini-2.0-flash'\n",
    "EMBEDDING_MODEL = 'BAAI/bge-m3'\n",
    "USE_QUERY_REWRITE = True     # LLM paraphrase & enrich truy vấn\n",
    "USE_COT_PLANNER   = True     # LLM đề xuất bước retrieve tiếp theo\n",
    "USE_LLM_RERANK    = True     # LLM chấm điểm lại các passage\n",
    "USE_PASSAGE_SUM   = True    # LLM tóm tắt passage (giảm độ dài prompt)\n",
    "LLM_RERANK_TOP_K  = 5       # Chỉ gửi tối đa K passage cho LLM reranker\n",
    "\n",
    "# Set random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install required packages\"\"\"\n",
    "    print(\"🔧 Installing dependencies...\")\n",
    "    subprocess.run(\"pip install -q --upgrade transformers\", shell=True)\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "    except ImportError:\n",
    "        subprocess.run(\"pip install -q google-generativeai\", shell=True)\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "    except ImportError:\n",
    "        subprocess.run(\"pip install -q --upgrade FlagEmbedding\", shell=True)\n",
    "\n",
    "install_dependencies()\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "try:\n",
    "    from FlagEmbedding import BGEM3FlagModel\n",
    "except ImportError:\n",
    "    BGEM3FlagModel = None\n",
    "\n",
    "SearchResult = namedtuple('SearchResult', ['passage', 'tree_hop_graph'])\n",
    "\n",
    "class TreeHopGraph:\n",
    "    \"\"\"TreeHop graph tracking\"\"\"\n",
    "    def __init__(self, query: str):\n",
    "        self.query = query\n",
    "        self.hops = []\n",
    "        self.passages = {}\n",
    "        self.edges = []\n",
    "        self.overlap_history = []\n",
    "        \n",
    "    def add_hop(self, hop_id: int, passages: List[Dict], query_change: float = 0.0, overlap_magnitude: float = 0.0):\n",
    "        hop_info = {\n",
    "            'hop_id': hop_id,\n",
    "            'passages': passages,\n",
    "            'query_embedding_change': query_change,\n",
    "            'overlap_magnitude': overlap_magnitude,\n",
    "            'num_passages': len(passages)\n",
    "        }\n",
    "        self.hops.append(hop_info)\n",
    "        \n",
    "        for p in passages:\n",
    "            p_id = p.get('id', p['title'])\n",
    "            self.passages[p_id] = p\n",
    "            self.edges.append((f\"query_hop_{hop_id}\", p_id))\n",
    "            \n",
    "        self.overlap_history.append(overlap_magnitude)\n",
    "\n",
    "class SimpleTreeHopGraph:\n",
    "    \"\"\"Simplified graph structure to simulate DGL functionality for TreeHop\"\"\"\n",
    "    def __init__(self, query_emb: np.ndarray):\n",
    "        self.nodes = {'rep': [query_emb], 'h': [query_emb]}  # node embeddings\n",
    "        self.edges = []  # (src, dst) pairs\n",
    "        self.query_history = [query_emb]\n",
    "        \n",
    "    def add_context_nodes(self, ctx_embs: List[np.ndarray], query_emb: np.ndarray):\n",
    "        \"\"\"Add context nodes and edges for next hop\"\"\"\n",
    "        start_idx = len(self.nodes['rep'])\n",
    "        \n",
    "        # Add context nodes\n",
    "        for ctx_emb in ctx_embs:\n",
    "            self.nodes['rep'].append(ctx_emb)\n",
    "            self.nodes['h'].append(query_emb)  # Initialize h with query\n",
    "            \n",
    "        # Add edges from query to each context\n",
    "        for i, _ in enumerate(ctx_embs):\n",
    "            self.edges.append((0, start_idx + i))  # Query node (0) to context node\n",
    "            \n",
    "    def get_latest_query_nodes(self):\n",
    "        \"\"\"Get the most recent query embeddings for branching\"\"\"\n",
    "        return [self.nodes['h'][i] for i in range(1, len(self.nodes['h']))]\n",
    "        \n",
    "    def update_query_embeddings(self, new_query_embs: List[np.ndarray]):\n",
    "        \"\"\"Update query embeddings after TreeHop processing\"\"\"\n",
    "        for i, new_emb in enumerate(new_query_embs, 1):\n",
    "            if i < len(self.nodes['h']):\n",
    "                self.nodes['h'][i] = new_emb\n",
    "                \n",
    "        # Track query evolution\n",
    "        if new_query_embs:\n",
    "            # Use the best query embedding (highest change)\n",
    "            best_emb = max(new_query_embs, key=lambda x: np.linalg.norm(x - self.query_history[-1]))\n",
    "            self.query_history.append(best_emb)\n",
    "\n",
    "def extract_numeric_from_complex_object(obj, target_shape=None):\n",
    "    \"\"\"Extract numeric data from BGE-m3 complex output\"\"\"\n",
    "    def recursive_extract(item):\n",
    "        if isinstance(item, (int, float)):\n",
    "            return float(item)\n",
    "        elif isinstance(item, np.ndarray):\n",
    "            return item.astype(np.float32)\n",
    "        elif isinstance(item, list):\n",
    "            try:\n",
    "                return np.array(item, dtype=np.float32)\n",
    "            except (ValueError, TypeError):\n",
    "                numeric_items = []\n",
    "                for subitem in item:\n",
    "                    extracted = recursive_extract(subitem)\n",
    "                    if extracted is not None:\n",
    "                        numeric_items.append(extracted)\n",
    "                if numeric_items:\n",
    "                    try:\n",
    "                        return np.array(numeric_items, dtype=np.float32)\n",
    "                    except:\n",
    "                        return numeric_items[0] if len(numeric_items) == 1 else np.concatenate(numeric_items)\n",
    "                return None\n",
    "        elif isinstance(item, dict):\n",
    "            for key in ['dense_vecs', 'dense', 'embeddings', 'vectors', 'data']:\n",
    "                if key in item:\n",
    "                    return recursive_extract(item[key])\n",
    "            for value in item.values():\n",
    "                result = recursive_extract(value)\n",
    "                if result is not None:\n",
    "                    return result\n",
    "            return None\n",
    "        elif hasattr(item, '__iter__') and not isinstance(item, str):\n",
    "            try:\n",
    "                return np.array(list(item), dtype=np.float32)\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        result = recursive_extract(obj)\n",
    "        \n",
    "        if result is None:\n",
    "            dim = target_shape[0] if target_shape else 1024\n",
    "            result = np.random.randn(dim).astype(np.float32)\n",
    "            return result / np.linalg.norm(result)\n",
    "        \n",
    "        if not isinstance(result, np.ndarray):\n",
    "            result = np.array(result, dtype=np.float32)\n",
    "        \n",
    "        if result.ndim > 1:\n",
    "            result = result.flatten()\n",
    "        \n",
    "        if target_shape and len(target_shape) > 0:\n",
    "            target_dim = target_shape[0]\n",
    "            if result.shape[0] != target_dim:\n",
    "                if result.shape[0] > target_dim:\n",
    "                    result = result[:target_dim]\n",
    "                else:\n",
    "                    result = np.pad(result, (0, target_dim - result.shape[0]), 'constant')\n",
    "        \n",
    "        norm = np.linalg.norm(result)\n",
    "        if norm > 0:\n",
    "            return result / norm\n",
    "        else:\n",
    "            result[0] = 1.0\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error extracting numeric data: {e}\")\n",
    "        dim = target_shape[0] if target_shape else 1024\n",
    "        fallback = np.random.randn(dim).astype(np.float32)\n",
    "        return fallback / np.linalg.norm(fallback)\n",
    "\n",
    "class CorrectedTreeHopRetriever:\n",
    "    \"\"\"TreeHop implementation corrected according to original repo\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, passages_file, embed_dim=1024, g_size=64, n_heads=3, mlp_size=64):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.g_size = g_size\n",
    "        self.n_heads = n_heads\n",
    "        self.mlp_size = mlp_size\n",
    "        self.model_name = model_name\n",
    "        self.passages_file = passages_file\n",
    "        self.passages = []\n",
    "        self.embeddings = None\n",
    "        self.embedding_model = None\n",
    "        \n",
    "        print(f\"🚀 Initializing CORRECTED TreeHop Retriever:\")\n",
    "        print(f\"  embed_dim: {embed_dim}, g_size: {g_size}\")\n",
    "        print(f\"  n_heads: {n_heads}, mlp_size: {mlp_size}\")\n",
    "        \n",
    "        self._init_embedding_model()\n",
    "        self._init_corrected_treehop_networks()\n",
    "        \n",
    "        self.stats = {\n",
    "            'total_queries': 0,\n",
    "            'total_hops': 0,\n",
    "            'overlap_applications': 0,\n",
    "            'avg_overlap_magnitude': 0.0\n",
    "        }\n",
    "    \n",
    "    def _init_embedding_model(self):\n",
    "        \"\"\"Initialize BGE-m3 model\"\"\"\n",
    "        print(\"🔧 Loading BGE-m3...\")\n",
    "        \n",
    "        if BGEM3FlagModel is not None:\n",
    "            try:\n",
    "                self.embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "                self.embed_dim = 1024\n",
    "                print(\"✅ BGE-m3 loaded successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"❌ BGE-m3 failed: {e}\")\n",
    "        \n",
    "        print(\"⚠️ Using fallback embedding\")\n",
    "        self.embedding_model = None\n",
    "    \n",
    "    def _init_corrected_treehop_networks(self):\n",
    "        \"\"\"Initialize TreeHop networks CORRECTED according to original repo\"\"\"\n",
    "        # AttentionHead2D weights for each head\n",
    "        self.attention_heads = []\n",
    "        for i in range(self.n_heads):\n",
    "            head_weights = {\n",
    "                'W_Q': np.random.randn(self.embed_dim, self.g_size) * 0.02,\n",
    "                'W_K': np.random.randn(self.embed_dim, self.g_size) * 0.02,\n",
    "                'W_V': np.random.randn(self.embed_dim, self.g_size) * 0.02,\n",
    "                # MultiMLPLayer weights\n",
    "                'mlp_layer1': np.random.randn(self.g_size, self.mlp_size) * 0.02,\n",
    "                'mlp_layer2': np.random.randn(self.mlp_size, self.g_size) * 0.02,\n",
    "                'mlp_bias1': np.random.randn(self.mlp_size) * 0.01,\n",
    "                'mlp_bias2': np.random.randn(self.g_size) * 0.01,\n",
    "                # ResNet layer norm weights (simulated)\n",
    "                'layer_norm_weight': np.ones(self.g_size),\n",
    "                'layer_norm_bias': np.zeros(self.g_size),\n",
    "            }\n",
    "            self.attention_heads.append(head_weights)\n",
    "        \n",
    "        # TreeHopNode.update_attn_scale: Linear(g_size * n_heads, embed_size, bias=False)\n",
    "        self.update_attn_scale_weights = np.random.randn(self.g_size * self.n_heads, self.embed_dim) * 0.02\n",
    "        \n",
    "        print(f\"✅ Initialized {self.n_heads} CORRECTED attention heads\")\n",
    "    \n",
    "    def encode_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Robust BGE-m3 encoding with improved error handling\"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            # Hash fallback\n",
    "            import hashlib\n",
    "            text_bytes = text.encode('utf-8')\n",
    "            hash_bytes = hashlib.md5(text_bytes).digest()\n",
    "            vec = np.array([float(b) for b in hash_bytes])\n",
    "            \n",
    "            if len(vec) < self.embed_dim:\n",
    "                vec = np.pad(vec, (0, self.embed_dim - len(vec)), 'constant')\n",
    "            else:\n",
    "                vec = vec[:self.embed_dim]\n",
    "            \n",
    "            return vec / np.linalg.norm(vec)\n",
    "        \n",
    "        try:\n",
    "            # Ensure text is valid and clean\n",
    "            if not text or not isinstance(text, str):\n",
    "                text = \"empty text\"\n",
    "            \n",
    "            # Clean and truncate text for better encoding\n",
    "            text = text.strip()\n",
    "            # Remove excessive whitespace and special chars that might confuse BGE-m3\n",
    "            text = ' '.join(text.split())\n",
    "            text = text[:4000]  # Reasonable limit for BGE-m3\n",
    "            \n",
    "            if hasattr(self.embedding_model, 'encode') and 'BAAI/bge-m3' in str(self.embedding_model):\n",
    "                for attempt in range(2):  # Reduced attempts for speed\n",
    "                    try:\n",
    "                        # Use most stable BGE-m3 configuration\n",
    "                        result = self.embedding_model.encode(\n",
    "                            [text], \n",
    "                            batch_size=1,\n",
    "                            max_length=1024,  # Optimal for BGE-m3\n",
    "                            return_dense=True, \n",
    "                            return_sparse=False, \n",
    "                            return_colbert_vecs=False,\n",
    "                            normalize_embeddings=True  # Important for similarity calculation\n",
    "                        )\n",
    "                        \n",
    "                        # Validate result is not None\n",
    "                        if result is None:\n",
    "                            print(f\"⚠️ BGE-m3 returned None for attempt {attempt+1}\")\n",
    "                            continue\n",
    "                        \n",
    "                        embedding = extract_numeric_from_complex_object(result, target_shape=(self.embed_dim,))\n",
    "                        \n",
    "                        # Validate embedding\n",
    "                        if embedding is not None and isinstance(embedding, np.ndarray) and embedding.shape[0] > 0:\n",
    "                            # Ensure proper normalization\n",
    "                            norm = np.linalg.norm(embedding)\n",
    "                            if norm > 0:\n",
    "                                return embedding / norm\n",
    "                            else:\n",
    "                                # Create normalized random vector if norm is 0\n",
    "                                embedding = np.random.randn(self.embed_dim).astype(np.float32)\n",
    "                                return embedding / np.linalg.norm(embedding)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ BGE-m3 attempt {attempt+1} failed: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(\"❌ All BGE-m3 attempts failed, using fallback\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Encoding error: {e}\")\n",
    "        \n",
    "        # Ultimate fallback - create a deterministic embedding based on text hash\n",
    "        import hashlib\n",
    "        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "        # Convert hex to numbers\n",
    "        hash_nums = [int(text_hash[i:i+2], 16) for i in range(0, min(len(text_hash), self.embed_dim*2), 2)]\n",
    "        \n",
    "        # Pad or truncate to correct dimension\n",
    "        if len(hash_nums) < self.embed_dim:\n",
    "            hash_nums.extend([0] * (self.embed_dim - len(hash_nums)))\n",
    "        else:\n",
    "            hash_nums = hash_nums[:self.embed_dim]\n",
    "        \n",
    "        fallback = np.array(hash_nums, dtype=np.float32)\n",
    "        return fallback / np.linalg.norm(fallback)\n",
    "    \n",
    "    def corrected_attention_head_2d(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, head_weights: dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        CORRECTED AttentionHead2D EXACTLY matching original TreeHop codebase\n",
    "        \"\"\"\n",
    "        # Linear transformations\n",
    "        Q_proj = np.dot(Q, head_weights['W_Q'])  # [embed_dim] -> [g_size]\n",
    "        K_proj = np.dot(K, head_weights['W_K'])  # [embed_dim] -> [g_size] \n",
    "        V_proj = np.dot(V, head_weights['W_V'])  # [embed_dim] -> [g_size]\n",
    "        \n",
    "        # CORRECTED: Exactly match original TreeHop\n",
    "        # Original: QK = Q * K (element-wise multiplication for 2D)\n",
    "        QK = Q_proj * K_proj  # Element-wise multiplication [g_size]\n",
    "        \n",
    "        # CORRECTED: Normalization exactly as original\n",
    "        # Original: scores = QK / Q.shape[1] ** 0.5  where Q.shape[1] is attn_size (g_size)\n",
    "        scores = QK / (self.g_size ** 0.5)\n",
    "        \n",
    "        # CORRECTED: Softmax exactly as original  \n",
    "        attn = np.exp(scores) / np.sum(np.exp(scores))  # Softmax activation\n",
    "        \n",
    "        # CORRECTED: Apply attention exactly as original\n",
    "        # Original: attn_out = self.dropout(attn) * V (no dropout in numpy version)\n",
    "        attn_out = attn * V_proj  # [g_size]\n",
    "        \n",
    "        # CORRECTED: MLP processing exactly as original\n",
    "        # MultiMLPLayer -> mlp_scale + residual connection\n",
    "        # Layer norm (simulated)\n",
    "        x_norm = (attn_out - np.mean(attn_out)) / (np.std(attn_out) + 1e-8)\n",
    "        x_norm = x_norm * head_weights['layer_norm_weight'] + head_weights['layer_norm_bias']\n",
    "        \n",
    "        # MLP layers exactly as original\n",
    "        mlp_hidden = np.maximum(0, np.dot(x_norm, head_weights['mlp_layer1']) + head_weights['mlp_bias1'])  # ReLU\n",
    "        mlp_out = np.dot(mlp_hidden, head_weights['mlp_layer2']) + head_weights['mlp_bias2']\n",
    "        \n",
    "        # CORRECTED: Final output exactly as original\n",
    "        # Original: return self.mlp_scale(mlp_out) + attn_out\n",
    "        return mlp_out + attn_out  # Direct residual connection as in original\n",
    "    \n",
    "    def corrected_multi_head_attention_2d(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        CORRECTED MultiHeadAttention2D\n",
    "        \"\"\"\n",
    "        head_outputs = []\n",
    "        for head_weights in self.attention_heads:\n",
    "            head_out = self.corrected_attention_head_2d(Q, K, V, head_weights)\n",
    "            head_outputs.append(head_out)\n",
    "        \n",
    "        # Concatenate all heads: [g_size * n_heads]\n",
    "        multi_head_out = np.concatenate(head_outputs)\n",
    "        return multi_head_out\n",
    "    \n",
    "    def corrected_overlap_subtraction(self, query_emb: np.ndarray, context_emb: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        CORRECTED TreeHop overlap subtraction formula according to original repo:\n",
    "        \n",
    "        From TreeHopNode.reduce_func:\n",
    "        Q = nodes.mailbox[\"q\"].clone().squeeze(1)  # query\n",
    "        K = nodes.data[\"rep\"]                      # context  \n",
    "        V_update = nodes.data[\"rep\"]               # context\n",
    "        \n",
    "        update_gate = self.update_gate(Q, K, V_update)\n",
    "        h = Q - K + self.update_attn_scale(update_gate)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate inputs\n",
    "            if query_emb is None or context_emb is None:\n",
    "                print(\"⚠️ None embedding in overlap subtraction, returning original query\")\n",
    "                return query_emb if query_emb is not None else np.random.randn(self.embed_dim), 0.0\n",
    "            \n",
    "            # Ensure embeddings are numpy arrays\n",
    "            if not isinstance(query_emb, np.ndarray):\n",
    "                query_emb = np.array(query_emb, dtype=np.float32)\n",
    "            if not isinstance(context_emb, np.ndarray):\n",
    "                context_emb = np.array(context_emb, dtype=np.float32)\n",
    "            \n",
    "            # Ensure proper dimensions\n",
    "            if query_emb.shape[0] != self.embed_dim:\n",
    "                print(f\"⚠️ Query embedding wrong shape: {query_emb.shape}, expected: ({self.embed_dim},)\")\n",
    "                query_emb = np.resize(query_emb, (self.embed_dim,))\n",
    "                query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "            \n",
    "            if context_emb.shape[0] != self.embed_dim:\n",
    "                print(f\"⚠️ Context embedding wrong shape: {context_emb.shape}, expected: ({self.embed_dim},)\")\n",
    "                context_emb = np.resize(context_emb, (self.embed_dim,))\n",
    "                context_emb = context_emb / np.linalg.norm(context_emb)\n",
    "            \n",
    "            # Step 1: CORRECTED update_gate computation\n",
    "            # self.update_gate = MultiHeadAttention2D(embed_size, g_size, mlp_size, ...)\n",
    "            update_gate = self.corrected_multi_head_attention_2d(query_emb, context_emb, context_emb)\n",
    "            \n",
    "            # Validate update_gate\n",
    "            if update_gate is None or not isinstance(update_gate, np.ndarray):\n",
    "                print(\"⚠️ Update gate failed, using fallback\")\n",
    "                update_gate = np.zeros(self.g_size * self.n_heads, dtype=np.float32)\n",
    "            \n",
    "            # Step 2: CORRECTED update_attn_scale transformation\n",
    "            # self.update_attn_scale = nn.Linear(g_size * n_head, embed_size, bias=False)\n",
    "            attention_update = np.dot(update_gate, self.update_attn_scale_weights)  # No bias in original\n",
    "            \n",
    "            # Validate attention_update\n",
    "            if attention_update is None or not isinstance(attention_update, np.ndarray):\n",
    "                print(\"⚠️ Attention update failed, using zero update\")\n",
    "                attention_update = np.zeros(self.embed_dim, dtype=np.float32)\n",
    "            \n",
    "            # Ensure correct shape\n",
    "            if attention_update.shape[0] != self.embed_dim:\n",
    "                attention_update = np.resize(attention_update, (self.embed_dim,))\n",
    "            \n",
    "            # Step 3: CORRECTED TreeHop formula: h = Q - K + attention_update\n",
    "            overlap_removed = query_emb - context_emb  # Q - K\n",
    "            overlap_magnitude = np.linalg.norm(overlap_removed)\n",
    "            updated_query = overlap_removed + attention_update  # Q - K + update_attn_scale(update_gate)\n",
    "            \n",
    "            # Normalize the updated query\n",
    "            norm = np.linalg.norm(updated_query)\n",
    "            if norm > 0:\n",
    "                updated_query = updated_query / norm\n",
    "            else:\n",
    "                print(\"⚠️ Zero norm in updated query, using original\")\n",
    "                updated_query = query_emb\n",
    "            \n",
    "            return updated_query, overlap_magnitude\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in overlap subtraction: {e}\")\n",
    "            # Return original query on error\n",
    "            return query_emb, 0.0\n",
    "    \n",
    "    def load_passages(self):\n",
    "        \"\"\"Load passages from JSONL file\"\"\"\n",
    "        if not self.passages:\n",
    "            try:\n",
    "                with open(self.passages_file, 'r', encoding='utf-8') as f:\n",
    "                    self.passages = [json.loads(line) for line in f]\n",
    "                print(f\"✅ Loaded {len(self.passages)} passages\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading passages: {e}\")\n",
    "                self.passages = []\n",
    "        return self.passages\n",
    "    \n",
    "    def multihop_search_passages(self, query, n_hop=2, top_n=5, redundant_pruning=True, \n",
    "                               layerwise_top_pruning=True, return_tree=True):\n",
    "        \"\"\"Complete multi-hop search with CORRECTED TreeHop\"\"\"\n",
    "        if not self.passages:\n",
    "            self.load_passages()\n",
    "        \n",
    "        if len(self.passages) == 0:\n",
    "            print(\"⚠️ No passages loaded\")\n",
    "            return SearchResult([[[]]]*n_hop, None)\n",
    "        \n",
    "        if isinstance(query, str):\n",
    "            queries = [query]\n",
    "        else:\n",
    "            queries = query\n",
    "        \n",
    "        print(f\"🔍 Starting CORRECTED TreeHop {n_hop}-hop retrieval for {len(queries)} queries\")\n",
    "        \n",
    "        all_hop_results = []\n",
    "        tree_hop_graphs = []\n",
    "        \n",
    "        for query_idx, q in enumerate(queries):\n",
    "            print(f\"\\n=== Processing Query {query_idx+1}/{len(queries)} ===\")\n",
    "            print(f\"Query: {q[:100]}...\")\n",
    "            \n",
    "            tree_graph = TreeHopGraph(q)\n",
    "            \n",
    "            # Get initial query embedding  \n",
    "            current_query_emb = self.encode_text(q)\n",
    "            print(f\"✅ Initial query embedding shape: {current_query_emb.shape}\")\n",
    "            \n",
    "            query_hop_results = []\n",
    "            seen_passage_ids = set()\n",
    "            \n",
    "            # TreeHop stopping criterion variables\n",
    "            prev_query_emb = current_query_emb.copy()\n",
    "            convergence_threshold = 0.01  # Stop if query change < threshold\n",
    "            quality_threshold = 0.3      # Stop if retrieval quality too low\n",
    "            \n",
    "            # Initialize TreeHop graph structure\n",
    "            treehop_graph = SimpleTreeHopGraph(current_query_emb)\n",
    "            \n",
    "            for hop in range(n_hop):\n",
    "                print(f\"\\n--- Hop {hop+1}/{n_hop} ---\")\n",
    "                \n",
    "                # Early stopping check (after first hop)\n",
    "                if hop > 0:\n",
    "                    query_change = np.linalg.norm(current_query_emb - prev_query_emb)\n",
    "                    if query_change < convergence_threshold:\n",
    "                        print(f\"⚡ Early stopping: query converged (change={query_change:.4f} < {convergence_threshold})\")\n",
    "                        break\n",
    "                    \n",
    "                    # Check retrieval quality (similarity scores)\n",
    "                    if len(query_hop_results) > 0 and len(query_hop_results[-1]) > 0:\n",
    "                        last_scores = scores[np.argsort(scores)[-len(query_hop_results[-1]):][::-1]]\n",
    "                        avg_score = np.mean(last_scores)\n",
    "                        if avg_score < quality_threshold:\n",
    "                            print(f\"⚡ Early stopping: low retrieval quality (avg_score={avg_score:.4f} < {quality_threshold})\")\n",
    "                            break\n",
    "                \n",
    "                prev_query_emb = current_query_emb.copy()\n",
    "                \n",
    "                # Encode passages if not cached\n",
    "                if self.embeddings is None:\n",
    "                    print(\"🔄 Encoding passages...\")\n",
    "                    passage_texts = [f\"Title: {p['title']}\\nContent: {p['text']}\" for p in self.passages]\n",
    "                    passage_embeddings = []\n",
    "                    \n",
    "                    for i, text in enumerate(tqdm(passage_texts, desc=\"Encoding\")):\n",
    "                        try:\n",
    "                            emb = self.encode_text(text)\n",
    "                            passage_embeddings.append(emb)\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error encoding passage {i}: {e}\")\n",
    "                            fallback = np.random.randn(self.embed_dim).astype(np.float32)\n",
    "                            passage_embeddings.append(fallback / np.linalg.norm(fallback))\n",
    "                    \n",
    "                    self.embeddings = np.array(passage_embeddings)\n",
    "                    print(f\"✅ Encoded {len(passage_embeddings)} passages\")\n",
    "                else:\n",
    "                    passage_embeddings = self.embeddings\n",
    "                \n",
    "                # Calculate similarities\n",
    "                scores = np.dot(passage_embeddings, current_query_emb) / (\n",
    "                    np.linalg.norm(passage_embeddings, axis=1) * np.linalg.norm(current_query_emb)\n",
    "                )\n",
    "                \n",
    "                # TreeHop hop weighting - more aggressive weighting for later hops\n",
    "                hop_weight = 1.0 + hop * 0.5  # Increased from 0.3 to 0.5\n",
    "                scores = scores * hop_weight\n",
    "                \n",
    "                # Apply query-specific boosting for key terms\n",
    "                if hop > 0:  # Boost scores for passages containing query keywords\n",
    "                    query_words = set(q.lower().split())\n",
    "                    for i, passage in enumerate(self.passages):\n",
    "                        passage_text = f\"{passage['title']} {passage['text']}\".lower()\n",
    "                        overlap_count = sum(1 for word in query_words if word in passage_text and len(word) > 3)\n",
    "                        if overlap_count > 0:\n",
    "                            scores[i] *= (1.0 + overlap_count * 0.1)\n",
    "                \n",
    "                # CORRECTED TreeHop Pruning Strategy\n",
    "                if layerwise_top_pruning and hop > 0:\n",
    "                    # Layer-wise top-K pruning: keep only top-K candidates from previous hop\n",
    "                    # This prevents exponential growth of query branches\n",
    "                    effective_top_n = min(top_n, max(3, top_n // (hop + 1)))  # Reduce as hops increase\n",
    "                    print(f\"  Layer-wise pruning: using top-{effective_top_n} for hop {hop+1}\")\n",
    "                else:\n",
    "                    effective_top_n = top_n\n",
    "                \n",
    "                # Select top passages with redundant pruning\n",
    "                if redundant_pruning and hop > 0:\n",
    "                    available_indices = [i for i, p in enumerate(self.passages) \n",
    "                                       if p.get('id', p['title']) not in seen_passage_ids]\n",
    "                    if len(available_indices) == 0:\n",
    "                        print(f\"⚠️ No new passages at hop {hop+1}\")\n",
    "                        break\n",
    "                    available_scores = scores[available_indices]\n",
    "                    top_indices_local = np.argsort(available_scores)[-effective_top_n:][::-1]\n",
    "                    top_indices = [available_indices[i] for i in top_indices_local]\n",
    "                else:\n",
    "                    top_indices = np.argsort(scores)[-effective_top_n:][::-1]\n",
    "                \n",
    "                # Get top passages\n",
    "                hop_passages = [self.passages[i] for i in top_indices]\n",
    "                hop_passage_embeddings = np.array([passage_embeddings[i] for i in top_indices])\n",
    "                \n",
    "                # Track seen passages\n",
    "                for p in hop_passages:\n",
    "                    seen_passage_ids.add(p.get('id', p['title']))\n",
    "                \n",
    "                print(f\"✅ Retrieved {len(hop_passages)} passages\")\n",
    "                \n",
    "                # Store results\n",
    "                query_hop_results.append(hop_passages)\n",
    "                \n",
    "                # Apply CORRECTED TreeHop overlap subtraction with BRANCHING\n",
    "                query_change_magnitude = 0.0\n",
    "                overlap_magnitude = 0.0\n",
    "                \n",
    "                if hop < n_hop - 1:  # Don't update after last hop\n",
    "                    # CORRECTED: TreeHop graph-based query branching exactly like original\n",
    "                    # Original uses DGL graphs with message passing between query and context nodes\n",
    "                    \n",
    "                    print(f\"🧠 Applying CORRECTED TreeHop graph-based message passing...\")\n",
    "                    print(f\"  Before: query norm = {np.linalg.norm(current_query_emb):.4f}\")\n",
    "                    \n",
    "                    # Add context nodes to graph (simulating DGL add_nodes)\n",
    "                    treehop_graph.add_context_nodes(hop_passage_embeddings.tolist(), current_query_emb)\n",
    "                    \n",
    "                    # TreeHop message passing: process each context node individually\n",
    "                    # This simulates the DGL reduce_func for each edge\n",
    "                    query_candidates = []\n",
    "                    overlap_magnitudes = []\n",
    "                    \n",
    "                    for i, passage_emb in enumerate(hop_passage_embeddings):\n",
    "                        # Simulate TreeHopNode.reduce_func:\n",
    "                        # Q = nodes.mailbox[\"q\"] (query from previous hop)\n",
    "                        # K = nodes.data[\"rep\"] (current context)\n",
    "                        # V_update = nodes.data[\"rep\"] (same as K)\n",
    "                        \n",
    "                        Q = current_query_emb  # Query from mailbox\n",
    "                        K = passage_emb        # Context representation  \n",
    "                        V_update = passage_emb # Same as K for V\n",
    "                        \n",
    "                        # Apply TreeHop formula: h = Q - K + self.update_attn_scale(update_gate)\n",
    "                        updated_query_i, overlap_mag_i = self.corrected_overlap_subtraction(Q, K)\n",
    "                        query_candidates.append(updated_query_i)\n",
    "                        overlap_magnitudes.append(overlap_mag_i)\n",
    "                    \n",
    "                    # Update graph with new query embeddings\n",
    "                    treehop_graph.update_query_embeddings(query_candidates)\n",
    "                    \n",
    "                    # TreeHop query selection strategy\n",
    "                    if len(query_candidates) > 0:\n",
    "                        # Original TreeHop: select query with maximum information gain\n",
    "                        query_changes = [np.linalg.norm(q - current_query_emb) for q in query_candidates]\n",
    "                        \n",
    "                        # Advanced selection: weighted combination of change + diversity\n",
    "                        query_scores = []\n",
    "                        for i, (q_cand, change) in enumerate(zip(query_candidates, query_changes)):\n",
    "                            # Score = change magnitude + diversity from other candidates\n",
    "                            diversity = np.mean([np.linalg.norm(q_cand - other) \n",
    "                                               for j, other in enumerate(query_candidates) if i != j])\n",
    "                            score = change + 0.1 * diversity if len(query_candidates) > 1 else change\n",
    "                            query_scores.append(score)\n",
    "                        \n",
    "                        best_idx = np.argmax(query_scores)\n",
    "                        updated_query_emb = query_candidates[best_idx]\n",
    "                        overlap_magnitude = overlap_magnitudes[best_idx]\n",
    "                        query_change_magnitude = query_changes[best_idx]\n",
    "                        \n",
    "                        print(f\"  Selected query branch {best_idx+1}/{len(query_candidates)} (score: {query_scores[best_idx]:.4f})\")\n",
    "                        print(f\"  After: query norm = {np.linalg.norm(updated_query_emb):.4f}\")\n",
    "                        print(f\"  Query change = {query_change_magnitude:.4f}\")\n",
    "                        print(f\"  Overlap magnitude = {overlap_magnitude:.4f}\")\n",
    "                        \n",
    "                        # Update current query for next hop\n",
    "                        current_query_emb = updated_query_emb\n",
    "                    else:\n",
    "                        print(\"⚠️ No valid query candidates generated\")\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    self.stats['overlap_applications'] += 1\n",
    "                    self.stats['avg_overlap_magnitude'] = (\n",
    "                        (self.stats['avg_overlap_magnitude'] * (self.stats['overlap_applications'] - 1) + overlap_magnitude) \n",
    "                        / self.stats['overlap_applications']\n",
    "                    )\n",
    "                \n",
    "                # Add hop to graph\n",
    "                tree_graph.add_hop(hop + 1, hop_passages, query_change_magnitude, overlap_magnitude)\n",
    "            \n",
    "            # Store results\n",
    "            all_hop_results.append(query_hop_results)\n",
    "            tree_hop_graphs.append(tree_graph)\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['total_queries'] += 1\n",
    "            self.stats['total_hops'] += len(query_hop_results)\n",
    "            \n",
    "            # Print summary\n",
    "            total_passages = sum(len(hop) for hop in query_hop_results)\n",
    "            print(f\"\\n✅ Query {query_idx+1} complete:\")\n",
    "            print(f\"  Hops: {len(query_hop_results)}\")\n",
    "            print(f\"  Total passages: {total_passages}\")\n",
    "            print(f\"  Unique passages: {len(seen_passage_ids)}\")\n",
    "        \n",
    "        return SearchResult(all_hop_results, tree_hop_graphs if return_tree else None)\n",
    "\n",
    "# Utility functions (same as before)\n",
    "def load_corpus(corpus_file: str) -> Dict[str, str]:\n",
    "    \"\"\"Load corpus\"\"\"\n",
    "    print(f\"📖 Loading corpus from {corpus_file}...\")\n",
    "    \n",
    "    corpus_data = {}\n",
    "    current_title = None\n",
    "    current_passage_lines = []\n",
    "\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Parsing corpus\"):\n",
    "            line = line.strip()\n",
    "\n",
    "            if \"<endofpassage>\" in line:\n",
    "                if current_title and current_passage_lines:\n",
    "                    corpus_data[current_title] = \" \".join(current_passage_lines).strip()\n",
    "                \n",
    "                parts = line.split(\"<endofpassage>\", 1)\n",
    "                if len(parts) > 1 and \"Title:\" in parts[1]:\n",
    "                    current_title = parts[1].replace(\"Title:\", \"\").strip()\n",
    "                    current_passage_lines = []\n",
    "                else:\n",
    "                    current_title = None\n",
    "                    current_passage_lines = []\n",
    "                continue\n",
    "\n",
    "            elif line.startswith(\"Title:\"):\n",
    "                if current_title and current_passage_lines:\n",
    "                    corpus_data[current_title] = \" \".join(current_passage_lines).strip()\n",
    "                \n",
    "                current_title = line.replace(\"Title:\", \"\").strip()\n",
    "                current_passage_lines = []\n",
    "                continue\n",
    "\n",
    "            elif line.startswith(\"Passage:\"):\n",
    "                passage_content = line.replace(\"Passage:\", \"\").strip()\n",
    "                if passage_content:\n",
    "                    current_passage_lines.append(passage_content)\n",
    "                continue\n",
    "                \n",
    "            elif current_title is not None:\n",
    "                current_passage_lines.append(line)\n",
    "\n",
    "    if current_title and current_passage_lines:\n",
    "        corpus_data[current_title] = \" \".join(current_passage_lines).strip()\n",
    "\n",
    "    print(f\"✅ Loaded {len(corpus_data)} documents\")\n",
    "    return corpus_data\n",
    "\n",
    "def load_queries(queries_file: str) -> List[Dict]:\n",
    "    \"\"\"Load queries\"\"\"\n",
    "    print(f\"📖 Loading queries from {queries_file}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(queries_file, 'r', encoding='utf-8') as f:\n",
    "            queries_data = json.load(f)\n",
    "        \n",
    "        print(f\"✅ Loaded {len(queries_data)} queries\")\n",
    "        return queries_data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading queries: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_passages_jsonl(corpus: Dict[str, str], output_file: str) -> List[Dict]:\n",
    "    \"\"\"Create passages JSONL\"\"\"\n",
    "    passages = []\n",
    "    for i, (title, text) in enumerate(corpus.items()):\n",
    "        passages.append({\n",
    "            \"id\": i,\n",
    "            \"title\": title,\n",
    "            \"text\": text\n",
    "        })\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for passage in passages:\n",
    "            f.write(json.dumps(passage, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✅ Created passages file at {output_file}\")\n",
    "    return passages\n",
    "\n",
    "def setup_gemini_model(api_key: str):\n",
    "    \"\"\"Setup Gemini API\"\"\"\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    return model\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Normalize answer\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set('.,:;!?()[]{}\\\\/»«\\'\"\"-')\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1(pred: str, gold: str) -> float:\n",
    "    \"\"\"Compute F1 score\"\"\"\n",
    "    normalized_pred = normalize_answer(pred)\n",
    "    normalized_gold = normalize_answer(gold)\n",
    "    \n",
    "    if normalized_pred == normalized_gold:\n",
    "        return 1.0\n",
    "    \n",
    "    pred_tokens = set(normalized_pred.split())\n",
    "    gold_tokens = set(normalized_gold.split())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    common_tokens = pred_tokens.intersection(gold_tokens)\n",
    "    \n",
    "    precision = len(common_tokens) / len(pred_tokens)\n",
    "    recall = len(common_tokens) / len(gold_tokens)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def multihop_qa(retriever: CorrectedTreeHopRetriever, qa_model, query_data: Dict, \n",
    "                n_hop=MAX_HOPS, top_n=TOP_N) -> Dict:\n",
    "    \"\"\"Multi-hop QA\"\"\"\n",
    "    query = query_data[\"query\"]\n",
    "    gold_answer = query_data[\"answer\"]\n",
    "    question_type = query_data.get(\"question_type\", \"unknown\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n🎯 === Multi-hop QA ===\")\n",
    "    print(f\"Question type: {question_type}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    try:\n",
    "        # TreeHop retrieval\n",
    "        retrieved_result = retriever.multihop_search_passages(\n",
    "            query,\n",
    "            n_hop=n_hop,\n",
    "            top_n=top_n,\n",
    "            redundant_pruning=REDUNDANT_PRUNING,\n",
    "            layerwise_top_pruning=LAYERWISE_TOP_PRUNING,\n",
    "            return_tree=True\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        if not retrieved_result.passage or not retrieved_result.passage[0]:\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"gold_answer\": gold_answer,\n",
    "                \"pred_answer\": \"No relevant passages found\",\n",
    "                \"f1_score\": 0.0,\n",
    "                \"iterations\": 0,\n",
    "                \"retrieval_time\": time.time() - start_time,\n",
    "                \"num_passages\": 0,\n",
    "                \"question_type\": question_type,\n",
    "                \"tree_hop_graph\": None,\n",
    "            }\n",
    "        \n",
    "        # Collect all passages\n",
    "        all_passages = []\n",
    "        for hop_passages in retrieved_result.passage[0]:\n",
    "            all_passages.extend(hop_passages)\n",
    "        \n",
    "        # Generate answer with Gemini\n",
    "        pred_answer, iterations = generate_answer(qa_model, query, all_passages)\n",
    "        \n",
    "        # Compute F1 score\n",
    "        f1 = compute_f1(pred_answer, gold_answer)\n",
    "        \n",
    "        print(f\"💡 Gold answer: {gold_answer}\")\n",
    "        print(f\"🤖 Predicted answer: {pred_answer}\")\n",
    "        print(f\"📊 F1 score: {f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"pred_answer\": pred_answer,\n",
    "            \"f1_score\": f1,\n",
    "            \"iterations\": len(retrieved_result.passage[0]) if retrieved_result.passage else 0,\n",
    "            \"retrieval_time\": time.time() - start_time,\n",
    "            \"num_passages\": len(all_passages),\n",
    "            \"question_type\": question_type,\n",
    "            \"tree_hop_graph\": retrieved_result.tree_hop_graph[0] if retrieved_result.tree_hop_graph else None,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in multi-hop QA: {e}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"pred_answer\": f\"Error: {str(e)}\",\n",
    "            \"f1_score\": 0.0,\n",
    "            \"iterations\": 0,\n",
    "            \"retrieval_time\": time.time() - start_time,\n",
    "            \"num_passages\": 0,\n",
    "            \"question_type\": question_type,\n",
    "            \"tree_hop_graph\": None,\n",
    "        }\n",
    "\n",
    "def generate_answer(model, query: str, contexts: List[Dict], max_retries=3) -> Tuple[str, int]:\n",
    "    \"\"\"Generate answer using Gemini\"\"\"\n",
    "    context_texts = []\n",
    "    for idx, ctx in enumerate(contexts, 1):\n",
    "        if isinstance(ctx, dict) and 'title' in ctx and 'text' in ctx:\n",
    "            context_texts.append(f\"[PASSAGE {idx}]\\nTITLE: {ctx['title']}\\nCONTENT: {ctx['text']}\")\n",
    "    \n",
    "    all_contexts = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    \n",
    "    prompt = f\"\"\"Answer the question based on the provided passages retrieved using TreeHop multi-hop reasoning.\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "RETRIEVED INFORMATION:\n",
    "{all_contexts}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Provide ONLY the direct answer requested - no explanations or extra text\n",
    "- For \"Who\" questions: provide ONLY the person's name (e.g., \"Sam Bankman-Fried\")\n",
    "- For \"Which company\" questions: provide ONLY the company name (e.g., \"Google\") \n",
    "- For Yes/No questions: respond with ONLY \"Yes\" or \"No\"\n",
    "- If comparing articles, answer ONLY \"Yes\" or \"No\" based on the comparison\n",
    "- Extract the exact answer from the passages - do not paraphrase\n",
    "- Only respond \"Insufficient information\" if NO passages contain ANY relevant information\n",
    "\n",
    "ANSWER (one word/phrase only):\"\"\"\n",
    "    \n",
    "    print(f\"🤖 Generating answer with Gemini...\")\n",
    "    \n",
    "    iterations = 0\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            generation_config = {\n",
    "                \"temperature\": 0.05,\n",
    "                \"top_p\": 0.95,\n",
    "                \"top_k\": 40,\n",
    "                \"max_output_tokens\": 100,\n",
    "            }\n",
    "            \n",
    "            safety_settings = [\n",
    "                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "            ]\n",
    "            \n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            iterations += 1\n",
    "            answer = response.text.strip()\n",
    "            \n",
    "            if \"insufficient information\" in answer.lower():\n",
    "                answer = \"Insufficient information.\"\n",
    "            \n",
    "            return answer, iterations\n",
    "            \n",
    "        except Exception as e:\n",
    "            iterations += 1\n",
    "            print(f\"⚠️ Error generating answer (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return \"Error generating answer\", iterations\n",
    "\n",
    "def run_evaluation(retriever, qa_model, test_queries, n_hop=MAX_HOPS, top_n=TOP_N, max_samples=None) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Run evaluation\"\"\"\n",
    "    if max_samples and max_samples < len(test_queries):\n",
    "        print(f\"🎯 Running evaluation on {max_samples} samples from {len(test_queries)} total queries\")\n",
    "        test_subset = test_queries[:max_samples]\n",
    "    else:\n",
    "        test_subset = test_queries\n",
    "        print(f\"🎯 Running evaluation on all {len(test_subset)} test queries\")\n",
    "    \n",
    "    results = []\n",
    "    metrics = {\n",
    "        \"question_type\": defaultdict(list),\n",
    "        \"overall\": {\"f1\": [], \"iterations\": [], \"time\": [], \"passages\": []}\n",
    "    }\n",
    "    \n",
    "    # Process each query\n",
    "    for i, query_data in enumerate(tqdm(test_subset, desc=\"Processing test queries\")):\n",
    "        print(f\"\\n[{i+1}/{len(test_subset)}] Processing query: {query_data['query'][:100]}...\")\n",
    "        \n",
    "        # Run multi-hop QA\n",
    "        result = multihop_qa_llm(retriever, qa_model, query_data,\n",
    "                         n_hop=n_hop, top_n=top_n)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        question_type = query_data.get(\"question_type\", \"unknown\")\n",
    "        metrics[\"question_type\"][question_type].append({\n",
    "            \"f1\": result[\"f1_score\"],\n",
    "            \"iterations\": result[\"iterations\"],\n",
    "            \"time\": result[\"retrieval_time\"],\n",
    "            \"passages\": result[\"num_passages\"]\n",
    "        })\n",
    "        \n",
    "        metrics[\"overall\"][\"f1\"].append(result[\"f1_score\"])\n",
    "        metrics[\"overall\"][\"iterations\"].append(result[\"iterations\"])\n",
    "        metrics[\"overall\"][\"time\"].append(result[\"retrieval_time\"])\n",
    "        metrics[\"overall\"][\"passages\"].append(result[\"num_passages\"])\n",
    "        \n",
    "        # Print progressive results\n",
    "        if (i+1) % 5 == 0:\n",
    "            current_f1 = mean(metrics[\"overall\"][\"f1\"])\n",
    "            current_iterations = mean(metrics[\"overall\"][\"iterations\"]) \n",
    "            print(f\"\\n📊 Interim results after {i+1} samples:\")\n",
    "            print(f\"  Current average F1: {current_f1:.4f}\")\n",
    "            print(f\"  Current average iterations: {current_iterations:.2f}\")\n",
    "    \n",
    "    return results, metrics\n",
    "\n",
    "def _call_llm(model, prompt: str,\n",
    "              temperature: float = 0.2,\n",
    "              max_tokens: int = 64,\n",
    "              top_p: float = 0.95,\n",
    "              top_k: int = 40) -> str:\n",
    "    \"\"\"Gọi LLM một cách an toàn – luôn trả string (kể cả khi lỗi)  \n",
    "       + throttle 20 s giữa các lần gọi.\"\"\"\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"top_k\": top_k,\n",
    "                \"max_output_tokens\": max_tokens,\n",
    "            }\n",
    "        )\n",
    "        return (resp.text or \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  LLM call failed: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        time.sleep(3)\n",
    "\n",
    "# ========= 1. Query Rewriter ===============================================\n",
    "def rewrite_query_with_llm(llm_model, original_query: str) -> str:\n",
    "    \"\"\"Paraphrase + thêm từ khóa ngữ nghĩa cho query gốc.\"\"\"\n",
    "    prompt = (f\"Rewrite the question below into a single, clearer sentence. \"\n",
    "              f\"Add synonyms or context words if it helps retrieval.\\n\\n\"\n",
    "              f\"QUESTION: {original_query}\\n\\n\"\n",
    "              f\"IMPROVED QUESTION:\")\n",
    "    improved = _call_llm(llm_model, prompt, temperature=0.6, max_tokens=40)\n",
    "    return improved or original_query  # fallback-safe\n",
    "\n",
    "# ========= 2. Chain-of-Thought Planner =====================================\n",
    "def plan_next_hop(llm_model, query: str, hop_summaries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Nhờ LLM đề xuất 'nên truy xuất thông tin gì tiếp' dựa trên lịch sử hops.\n",
    "    Trả về 1 câu ngắn – sẽ được encode rồi dùng như query phụ.\n",
    "    \"\"\"\n",
    "    history = \"\\n\".join(f\"Hop {i+1}: {s}\" for i, s in enumerate(hop_summaries))\n",
    "    prompt = (f\"You are a retrieval planner. Original question:\\n{query}\\n\\n\"\n",
    "              f\"Context seen so far:\\n{history}\\n\\n\"\n",
    "              f\"In ONE short sentence, state what information should be retrieved next \"\n",
    "              f\"to advance toward the answer.\")\n",
    "    return _call_llm(llm_model, prompt, temperature=0.5, max_tokens=30)\n",
    "\n",
    "# ========= 3. Passage Summarizer ===========================================\n",
    "def summarize_passage(llm_model, passage: Dict) -> str:\n",
    "    \"\"\"Tóm tắt 1 passage < 30 token, giữ fact chính để giảm prompt length.\"\"\"\n",
    "    prompt = (f\"Summarize the key fact(s) of the passage below in ONE sentence \"\n",
    "              f\"(≤ 30 tokens).\\n\\nTitle: {passage['title']}\\n\\n{passage['text']}\")\n",
    "    return _call_llm(llm_model, prompt, temperature=0.3, max_tokens=50)\n",
    "\n",
    "# ========= 4. LLM Reranker ==================================================\n",
    "def rerank_passages(llm_model, query: str,\n",
    "                    passages: List[Dict], top_k: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chấm điểm 1-10 mức liên quan giữa passage & query rồi sắp xếp lại.\n",
    "    Chỉ gửi TOP_K (đã sort cosine) để tiết kiệm token.\n",
    "    \"\"\"\n",
    "    candidates = passages[:top_k]\n",
    "    scored: List[tuple] = []\n",
    "    for p in candidates:\n",
    "        prompt = (f\"Question: {query}\\n\\n\"\n",
    "                  f\"Passage title: {p['title']}\\n\"\n",
    "                  f\"Passage: {p['text']}\\n\\n\"\n",
    "                  f\"Score from 1 (irrelevant) to 10 (directly answers):\")\n",
    "        score_txt = _call_llm(llm_model, prompt, temperature=0.0,\n",
    "                              max_tokens=4)\n",
    "        try:\n",
    "            score = float(score_txt.split()[0])\n",
    "        except ValueError:\n",
    "            score = 5.0\n",
    "        scored.append((score, p))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [p for _, p in scored]\n",
    "\n",
    "# ========= 5. Answer Verifier ==============================================\n",
    "def verify_answer(llm_model, query: str, answer: str, passages: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    LLM kiểm chứng câu trả lời cuối (Yes / No / Corrected …).\n",
    "    Trả lại answer đã chỉnh (hoặc giữ nguyên nếu đã ổn).\n",
    "    \"\"\"\n",
    "    ctx = \"\\n\\n\".join(f\"{p['title']}: {p['text']}\" for p in passages[:6])\n",
    "    prompt = (f\"Based only on the passages below, verify whether the ANSWER is fully \"\n",
    "              f\"supported. If wrong, replace with the correct concise answer. If \"\n",
    "              f\"cannot decide, respond 'Insufficient information'.\\n\\n\"\n",
    "              f\"QUESTION: {query}\\nANSWER: {answer}\\n\\nPASSAGES:\\n{ctx}\\n\\n\"\n",
    "              f\"VERIFIED ANSWER:\")\n",
    "    verified = _call_llm(llm_model, prompt, temperature=0.0, max_tokens=50)\n",
    "    return verified or answer\n",
    "\n",
    "# ========= 6. Wrapper: Enhanced TreeHop Retrieval ==========================\n",
    "def enhanced_multihop_search(retriever, llm_model, query: str,\n",
    "                             n_hop: int = 3, top_n: int = 10,\n",
    "                             use_rewrite=True, use_planner=True,\n",
    "                             use_rerank=True, use_summary=True):\n",
    "    \"\"\"\n",
    "    • B1  (optional)  Query rewrite.  \n",
    "    • B2            TreeHop multihop search.  \n",
    "    • B3  (optional) Planner → targeted extra hop.  \n",
    "    • B4  (optional) LLM rerank.  \n",
    "    • B5  (optional) Summaries (trả về cho downstream).\n",
    "    \"\"\"\n",
    "    # ---- rewrite ----------------------------------------------------------\n",
    "    run_query = rewrite_query_with_llm(llm_model, query) if use_rewrite else query\n",
    "    # ---- TreeHop retrieval ------------------------------------------------\n",
    "    base = retriever.multihop_search_passages(run_query, n_hop, top_n,\n",
    "                                              REDUNDANT_PRUNING,\n",
    "                                              LAYERWISE_TOP_PRUNING,\n",
    "                                              return_tree=False)\n",
    "    query_hops = base.passage[0]            # ← LẤY lớp thứ hai\n",
    "    passages   = sum(query_hops, [])        # flatten tất cả hops\n",
    "    # ---- planner & extra hop ---------------------------------------------\n",
    "    if use_planner:\n",
    "        hop_sums = [\", \".join(p['title'] for p in hop)  # dùng query_hops\n",
    "                for hop in query_hops]\n",
    "        next_hint = plan_next_hop(llm_model, run_query, hop_sums)\n",
    "        if next_hint:\n",
    "            extra = retriever.multihop_search_passages(next_hint, 1, top_n,\n",
    "                                                       False, False,\n",
    "                                                       return_tree=False)\n",
    "            passages.extend(extra.passage[0])\n",
    "    # ---- rerank -----------------------------------------------------------\n",
    "    if use_rerank and passages:\n",
    "        passages = rerank_passages(llm_model, run_query, passages, top_k=top_n)\n",
    "    # ---- summary ----------------------------------------------------------\n",
    "    summaries = []\n",
    "    if use_summary:\n",
    "        summaries = [summarize_passage(llm_model, p) for p in passages[:top_n]]\n",
    "    # Return enriched query, summaries, passages (đã rerank)\n",
    "    return run_query, summaries, passages[:top_n]\n",
    "\n",
    "def multihop_qa_llm(retriever: CorrectedTreeHopRetriever,\n",
    "                    qa_model,\n",
    "                    query_data: Dict,\n",
    "                    n_hop=MAX_HOPS,\n",
    "                    top_n=TOP_N) -> Dict:\n",
    "    \"\"\"\n",
    "    Nếu mọi USE_* = False ➜ hành xử y hệt multihop_qa gốc.\n",
    "    Nếu bật cờ ➜ dùng enhanced_multihop_search (TreeHop + LLM modules).\n",
    "    \"\"\"\n",
    "    query        = query_data[\"query\"]\n",
    "    gold_answer  = query_data[\"answer\"]\n",
    "    q_type       = query_data.get(\"question_type\", \"unknown\")\n",
    "    t0           = time.time()\n",
    "\n",
    "    # ---------------- TreeHop retrieval ----------------\n",
    "    if any([USE_QUERY_REWRITE, USE_COT_PLANNER, USE_LLM_RERANK, USE_PASSAGE_SUM]):\n",
    "        # TreeHop + LLM modules\n",
    "        run_q, summaries, passages = enhanced_multihop_search(\n",
    "            retriever, qa_model, query,\n",
    "            n_hop=n_hop, top_n=top_n,\n",
    "            use_rewrite   = USE_QUERY_REWRITE,\n",
    "            use_planner   = USE_COT_PLANNER,\n",
    "            use_rerank    = USE_LLM_RERANK,\n",
    "            use_summary   = USE_PASSAGE_SUM\n",
    "        )\n",
    "        tree_graph = None        # vì enhanced_multihop_search trả về list passage\n",
    "        hop_iters  = n_hop       # gần đúng\n",
    "    else:\n",
    "        # TreeHop thuần\n",
    "        result     = retriever.multihop_search_passages(\n",
    "            query, n_hop=n_hop, top_n=top_n,\n",
    "            redundant_pruning=REDUNDANT_PRUNING,\n",
    "            layerwise_top_pruning=LAYERWISE_TOP_PRUNING,\n",
    "            return_tree=True\n",
    "        )\n",
    "        passages   = sum(result.passage[0], [])\n",
    "        summaries  = []\n",
    "        tree_graph = result.tree_hop_graph[0] if result.tree_hop_graph else None\n",
    "        hop_iters  = len(result.passage[0]) if result.passage else 0\n",
    "\n",
    "    if not passages:\n",
    "        return {\n",
    "            \"query\": query, \"gold_answer\": gold_answer,\n",
    "            \"pred_answer\": \"No relevant passages found\", \"f1_score\": 0.0,\n",
    "            \"iterations\": hop_iters, \"retrieval_time\": time.time()-t0,\n",
    "            \"num_passages\": 0, \"question_type\": q_type,\n",
    "            \"tree_hop_graph\": tree_graph,\n",
    "        }\n",
    "\n",
    "    # ---------------- Gemini answerer ------------------\n",
    "    pred_answer, _ = generate_answer(qa_model, query, passages)\n",
    "\n",
    "    # (tùy chọn) Verify answer bằng LLM\n",
    "    # pred_answer = verify_answer(qa_model, query, pred_answer, passages)\n",
    "\n",
    "    f1 = compute_f1(pred_answer, gold_answer)\n",
    "\n",
    "    return {\n",
    "        \"query\": query, \"gold_answer\": gold_answer, \"pred_answer\": pred_answer,\n",
    "        \"f1_score\": f1, \"iterations\": hop_iters,\n",
    "        \"retrieval_time\": time.time()-t0, \"num_passages\": len(passages),\n",
    "        \"question_type\": q_type, \"tree_hop_graph\": tree_graph,\n",
    "        \"summaries\": summaries          # thêm field summaries nếu cần\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"🎯 === CORRECTED TreeHop Multi-hop QA ===\")\n",
    "    \n",
    "    # File paths\n",
    "    corpus_file = os.path.join(DATASET_DIR, CORPUS_FILENAME)\n",
    "    queries_file = os.path.join(DATASET_DIR, QUERIES_FILENAME)\n",
    "    \n",
    "    print(f\"📁 Looking for files:\")\n",
    "    print(f\"  Corpus: {corpus_file}\")\n",
    "    print(f\"  Queries: {queries_file}\")\n",
    "    \n",
    "    # Check files exist\n",
    "    if not os.path.exists(corpus_file):\n",
    "        print(f\"❌ Corpus file not found: {corpus_file}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(queries_file):\n",
    "        print(f\"❌ Queries file not found: {queries_file}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"\\n📖 Loading corpus and queries...\")\n",
    "        corpus = load_corpus(corpus_file)\n",
    "        queries = load_queries(queries_file)\n",
    "        \n",
    "        print(f\"✅ Loaded {len(corpus)} documents and {len(queries)} queries\")\n",
    "        \n",
    "        # Prepare passages\n",
    "        print(\"\\n📝 Preparing passages...\")\n",
    "        passages_file = os.path.join(OUTPUT_DIR, \"passages.jsonl\")\n",
    "        passages = prepare_passages_jsonl(corpus, passages_file)\n",
    "        \n",
    "        # Initialize CORRECTED TreeHop retriever\n",
    "        print(\"\\n🧠 Initializing CORRECTED TreeHop...\")\n",
    "        retriever = CorrectedTreeHopRetriever(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            passages_file=passages_file,\n",
    "            embed_dim=1024,  # BGE-m3 dimension\n",
    "            g_size=64,\n",
    "            n_heads=3,\n",
    "            mlp_size=64\n",
    "        )\n",
    "        \n",
    "        # Setup Gemini\n",
    "        print(\"\\n🤖 Setting up Gemini API...\")\n",
    "        api_key = os.environ.get(\"GEMINI_API_KEY\", None)\n",
    "        if not api_key:\n",
    "            print(\"⚠️ WARNING: No GEMINI_API_KEY environment variable found.\")\n",
    "            print(\"Please set your API key: export GEMINI_API_KEY='your_key_here'\")\n",
    "            api_key = input(\"Or enter your Gemini API key now: \")\n",
    "            if not api_key:\n",
    "                print(\"❌ No API key provided, cannot continue\")\n",
    "                return\n",
    "        \n",
    "        qa_model = setup_gemini_model(api_key)\n",
    "        \n",
    "        # Split data\n",
    "        random.shuffle(queries)\n",
    "        split_idx = int(len(queries) * TRAIN_RATIO)\n",
    "        train_queries = queries[:split_idx]\n",
    "        test_queries = queries[split_idx:]\n",
    "        \n",
    "        print(f\"📊 Split into {len(train_queries)} train and {len(test_queries)} test queries\")\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(\"\\n🎯 Running CORRECTED TreeHop evaluation...\")\n",
    "        max_test_samples = min(len(test_queries), 10)  # Increased from 20 to 100\n",
    "        \n",
    "        results, metrics = run_evaluation(\n",
    "            retriever, qa_model, test_queries, \n",
    "            n_hop=MAX_HOPS, top_n=TOP_N,\n",
    "            max_samples=max_test_samples\n",
    "        )\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        avg_f1 = mean(metrics[\"overall\"][\"f1\"])\n",
    "        avg_iterations = mean(metrics[\"overall\"][\"iterations\"])\n",
    "        avg_time = mean(metrics[\"overall\"][\"time\"])\n",
    "        \n",
    "        print(f\"\\n📊 === Final Results (CORRECTED TreeHop) ===\")\n",
    "        print(f\"🎯 Average F1 Score: {avg_f1:.4f}\")\n",
    "        print(f\"🔄 Average Iterations: {avg_iterations:.2f}\")\n",
    "        print(f\"⏱️ Average Time: {avg_time:.3f}s\")\n",
    "        print(f\"🧠 TreeHop Statistics:\")\n",
    "        print(f\"    Queries processed: {retriever.stats['total_queries']}\")\n",
    "        print(f\"    Hops executed: {retriever.stats['total_hops']}\")\n",
    "        print(f\"    Overlap applications: {retriever.stats['overlap_applications']}\")\n",
    "        print(f\"    Average overlap magnitude: {retriever.stats['avg_overlap_magnitude']:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(OUTPUT_DIR, \"treehop_qa_results_corrected.json\")\n",
    "        \n",
    "        # Convert TreeHopGraph objects to dictionaries for JSON serialization\n",
    "        serializable_results = []\n",
    "        for result in results:\n",
    "            serializable_result = result.copy()\n",
    "            # Convert numpy types to Python types\n",
    "            for key, value in serializable_result.items():\n",
    "                if isinstance(value, np.floating):\n",
    "                    serializable_result[key] = float(value)\n",
    "                elif isinstance(value, np.integer):\n",
    "                    serializable_result[key] = int(value)\n",
    "            \n",
    "            if 'tree_hop_graph' in serializable_result and serializable_result['tree_hop_graph']:\n",
    "                graph = serializable_result['tree_hop_graph']\n",
    "                # Convert all numpy types in graph data\n",
    "                def convert_numpy(obj):\n",
    "                    if isinstance(obj, np.floating):\n",
    "                        return float(obj)\n",
    "                    elif isinstance(obj, np.integer):\n",
    "                        return int(obj)\n",
    "                    elif isinstance(obj, list):\n",
    "                        return [convert_numpy(item) for item in obj]\n",
    "                    elif isinstance(obj, dict):\n",
    "                        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "                    return obj\n",
    "                \n",
    "                serializable_result['tree_hop_graph'] = {\n",
    "                    'query': graph.query,\n",
    "                    'hops': convert_numpy(graph.hops),\n",
    "                    'passages': convert_numpy(graph.passages),\n",
    "                    'edges': convert_numpy(graph.edges),\n",
    "                    'overlap_history': convert_numpy(graph.overlap_history)\n",
    "                }\n",
    "            serializable_results.append(serializable_result)\n",
    "        \n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'results': serializable_results,\n",
    "                'metrics': {\n",
    "                    'avg_f1': avg_f1,\n",
    "                    'avg_iterations': avg_iterations,\n",
    "                    'avg_time': avg_time,\n",
    "                    'treehop_stats': retriever.stats\n",
    "                }\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n✅ Results saved to {results_file}\")\n",
    "        print(\"🎉 CORRECTED TreeHop evaluation finished successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Critical Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7490935,
     "sourceId": 11915546,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "treehop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
